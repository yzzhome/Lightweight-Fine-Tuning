{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: GPT-2\n",
    "* Evaluation approach:  Hugging Face Trainer\n",
    "* Fine-tuning dataset: yelp_review_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e2f312-63f3-4126-a367-6fad8537521c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: peft in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (0.11.1)\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (4.24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (1.23.5)\n",
      "Requirement already satisfied: safetensors in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (22.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (from peft) (2.4.0.dev20240527+cu124)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (4.64.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (from peft) (0.30.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (from peft) (0.23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.17.0->peft) (4.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.9.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.13.0->peft) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers->peft) (0.11.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers->peft) (2022.7.9)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\yuzha\\appdata\\roaming\\python\\python310\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.2.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, EvalPrediction, DataCollatorWithPadding,GPT2LMHeadModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from peft import LoraConfig, PeftModelForSequenceClassification, TaskType, AutoPeftModelForSequenceClassification\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ca2be1f-6000-452f-a962-28b9ac751231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c9cd12-9296-4704-88bf-04d1c473ba13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuzha\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "270efa1c-079b-4941-9d54-a1fd0adb1fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852524d2b75d47a0bd5784d070376783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cd3fe004a349028fa4912da75f7623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "tokenized_ds = {}\n",
    "splits = [\"train\", \"test\"]      \n",
    "\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = dataset[split].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "755cb4e7-c24a-493c-b865-4dd4e0e2921a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuzha\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\yuzha/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\yuzha/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\model.safetensors\n",
      "All model checkpoint weights were used when initializing GPT2ForSequenceClassification.\n",
      "\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=len(set(tokenized_ds['train']['label'])))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Unfreeze all the model parameters.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3568e231-f0c4-4596-a9d0-dc41beb218d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_random_subset(dataset, fraction=0.1):\n",
    "    total_size = len(dataset)\n",
    "    subset_size = int(total_size * fraction)\n",
    "    indices = random.sample(range(total_size), subset_size)\n",
    "    return dataset.select(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9bdb27-63f6-48cc-bbbb-d755b59475f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def compute_metrics(p: EvalPrediction) -> Dict[str, float]:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    accuracy = accuracy_score(p.label_ids, preds)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3151858-4ded-41c9-8ef3-7e58537efe3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\yuzha\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:22: UserWarning: torch.cuda.amp.GradScaler(args...) is deprecated. Please use torch.amp.GradScaler('cuda', args...) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=16,\n",
    "\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with compute_metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=get_random_subset(tokenized_ds['train']),\n",
    "    eval_dataset=get_random_subset(tokenized_ds['test']),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48c7046b-c534-44b8-8aae-4592999d181f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 65000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12189\n",
      "  Number of trainable parameters = 124443648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12189' max='12189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12189/12189 33:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.769300</td>\n",
       "      <td>0.848414</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>0.629972</td>\n",
       "      <td>0.630031</td>\n",
       "      <td>0.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703600</td>\n",
       "      <td>0.805206</td>\n",
       "      <td>0.646600</td>\n",
       "      <td>0.646214</td>\n",
       "      <td>0.648136</td>\n",
       "      <td>0.646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.587400</td>\n",
       "      <td>0.827290</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>0.651874</td>\n",
       "      <td>0.652019</td>\n",
       "      <td>0.652000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-4063\n",
      "Configuration saved in ./results\\checkpoint-4063\\config.json\n",
      "Model weights saved in ./results\\checkpoint-4063\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-4063\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-4063\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-8126\n",
      "Configuration saved in ./results\\checkpoint-8126\\config.json\n",
      "Model weights saved in ./results\\checkpoint-8126\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-8126\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-8126\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-12189\n",
      "Configuration saved in ./results\\checkpoint-12189\\config.json\n",
      "Model weights saved in ./results\\checkpoint-12189\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-12189\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-12189\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results\\checkpoint-8126 (score: 0.8052063584327698).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12189, training_loss=0.8468306853466268, metrics={'train_runtime': 2082.4964, 'train_samples_per_second': 93.638, 'train_steps_per_second': 5.853, 'total_flos': 2.54771232768e+16, 'train_loss': 0.8468306853466268, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cf8ae05-1991-47b7-a253-5c429a2d7dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.8052063584327698, 'eval_accuracy': 0.6466, 'eval_f1': 0.6462138905013258, 'eval_precision': 0.6481364077177657, 'eval_recall': 0.6466, 'eval_runtime': 75.8732, 'eval_samples_per_second': 65.899, 'eval_steps_per_second': 4.125, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5775fadf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuzha\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\yuzha/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\yuzha/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\model.safetensors\n",
      "All model checkpoint weights were used when initializing GPT2ForSequenceClassification.\n",
      "\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 298,752 || all params: 124,742,400 || trainable%: 0.2395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuzha\\AppData\\Roaming\\Python\\Python310\\site-packages\\peft\\tuners\\lora\\layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type = TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    lora_alpha=6,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# load origin pre trained gpt2 model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=len(set(tokenized_ds['train']['label'])))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "lora_model = PeftModelForSequenceClassification(model, peft_config)\n",
    "\n",
    "lora_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\yuzha\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:22: UserWarning: torch.cuda.amp.GradScaler(args...) is deprecated. Please use torch.amp.GradScaler('cuda', args...) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/lora_models\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_dir='./logs',\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=16,\n",
    "\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=get_random_subset(tokenized_ds['train']),\n",
    "    eval_dataset=get_random_subset(tokenized_ds['test']),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4d4c908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 65000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20320\n",
      "  Number of trainable parameters = 298752\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20320' max='20320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20320/20320 1:36:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.882800</td>\n",
       "      <td>0.898609</td>\n",
       "      <td>0.605200</td>\n",
       "      <td>0.605636</td>\n",
       "      <td>0.606753</td>\n",
       "      <td>0.605200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>0.890690</td>\n",
       "      <td>0.602600</td>\n",
       "      <td>0.600280</td>\n",
       "      <td>0.599953</td>\n",
       "      <td>0.602600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.866900</td>\n",
       "      <td>0.887805</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.600149</td>\n",
       "      <td>0.604298</td>\n",
       "      <td>0.602400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.885193</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>0.610357</td>\n",
       "      <td>0.612216</td>\n",
       "      <td>0.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.968500</td>\n",
       "      <td>0.881323</td>\n",
       "      <td>0.607800</td>\n",
       "      <td>0.604793</td>\n",
       "      <td>0.605248</td>\n",
       "      <td>0.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.929300</td>\n",
       "      <td>0.877990</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.608483</td>\n",
       "      <td>0.611392</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.929800</td>\n",
       "      <td>0.877892</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.610275</td>\n",
       "      <td>0.612028</td>\n",
       "      <td>0.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.864100</td>\n",
       "      <td>0.876288</td>\n",
       "      <td>0.612200</td>\n",
       "      <td>0.610400</td>\n",
       "      <td>0.611524</td>\n",
       "      <td>0.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.873643</td>\n",
       "      <td>0.612800</td>\n",
       "      <td>0.610637</td>\n",
       "      <td>0.610313</td>\n",
       "      <td>0.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>0.873322</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.612271</td>\n",
       "      <td>0.612691</td>\n",
       "      <td>0.613200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-2032\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-2032\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-2032\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-4064\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-4064\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-4064\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-6096\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-6096\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-6096\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-8128\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-8128\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-8128\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-10160\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-10160\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-10160\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-12192\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-12192\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-12192\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-14224\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-14224\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-14224\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-16256\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-16256\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-16256\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-18288\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-18288\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-18288\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/lora_models\\checkpoint-20320\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in ./results/lora_models\\checkpoint-20320\\tokenizer_config.json\n",
      "Special tokens file saved in ./results/lora_models\\checkpoint-20320\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/lora_models\\checkpoint-20320 (score: 0.8733215928077698).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20320, training_loss=0.893572951184483, metrics={'train_runtime': 5826.1637, 'train_samples_per_second': 111.566, 'train_steps_per_second': 3.488, 'total_flos': 8.52220182528e+16, 'train_loss': 0.893572951184483, 'epoch': 10.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b47abf88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.8733215928077698, 'eval_accuracy': 0.6132, 'eval_f1': 0.6122709780198415, 'eval_precision': 0.6126905632497591, 'eval_recall': 0.6132, 'eval_runtime': 79.8442, 'eval_samples_per_second': 62.622, 'eval_steps_per_second': 1.966, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained('model/lora_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19a13aa5-2ee3-4c31-8005-a19288cd580a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "863ec66e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\yuzha/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\yuzha/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\model.safetensors\n",
      "All model checkpoint weights were used when initializing GPT2ForSequenceClassification.\n",
      "\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained('model/lora_model', num_labels=len(set(tokenized_ds['train']['label'])))\n",
    "inference_model.config.pad_token_id = inference_model.config.eos_token_id\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = inference_model,\n",
    "    args = training_args,\n",
    "    eval_dataset = tokenized_ds['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer= tokenizer,\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 02:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.8746678233146667, 'eval_accuracy': 0.61432, 'eval_f1': 0.6130612331565982, 'eval_precision': 0.6123497366027331, 'eval_recall': 0.61432, 'eval_runtime': 198.6707, 'eval_samples_per_second': 251.673, 'eval_steps_per_second': 7.867}\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe70502e-4704-488e-ac68-20f32cff3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2comment_dict = {\n",
    "    0: \"0-1\",\n",
    "    1: \"1-2\",\n",
    "    2: \"2-3\",\n",
    "    3: \"3-4\",\n",
    "    4: \"4-5\"\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc96905a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=5, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=5, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(sentence: str) -> str:\n",
    "    device = inference_model.device\n",
    "    \n",
    "    input = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    \n",
    "    output = inference_model(**input)\n",
    "    logits = output.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_id = probabilities.argmax().item()\n",
    "    predicted_label = id2comment_dict[predicted_id]\n",
    "\n",
    "    return predicted_label\n",
    "    #return predicted_id\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inference_model.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'It's the worst meal I have ever had'\n",
      "Predicted label: 0-1\n"
     ]
    }
   ],
   "source": [
    "sentence = \"It's the worst meal I have ever had\"\n",
    "predicted_label = predict(sentence)\n",
    "print(f\"Sentence: '{sentence}'\\nPredicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'good service and nice view'\n",
      "Predicted label: 3-4\n"
     ]
    }
   ],
   "source": [
    "sentence = \"good service and nice view\"\n",
    "predicted_label = predict(sentence)\n",
    "print(f\"Sentence: '{sentence}'\\nPredicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d20ff4a-522f-4d2f-a4d2-70714b8632a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yelp Comments: Think Chuck E. Cheese for adults.  Skee Ball, video games pool tables.  Clean environment.  Good fun.\\n\\nUnfortunately, I went for a bite to eat and it was impossible to find anything good and healthy on the menu.  I ended up settling for spinach dip.  Sadly, they topped the dip off with horrible orange shredded cheese that appeared to have been popped in the microwave for a few seconds.  Blahhhh.  Trying to get something healthy, I ordered the apple pecan salad.  I swear the dressing came right out of the grocery store bottle.  I could barely eat the salad.  Too sweet.\\n\\nMy mom ordered a steak roll.....holy friedness!  The steak was more like hamburger fried with cheese and then stuffed into breading that was fried AGAIN!  Yowzer!  Artery clogger for sure.  \\n\\nI like the atmosphere.  I like the bar area.  Perhaps next time we'll just stop by for drinks instead.\n",
      "\n",
      "Score: 1\n",
      "\n",
      "Prediction: 1\n",
      "\n",
      "\n",
      "Yelp Comments: We came here on a Saturday night and luckily it wasn't as packed as I thought it would be. I love playing ticket games so luckily there were a lot of games to chose from and not many video games (sorry video game lovers!) We only played $20 worth of games and surprisingly we were there for awhile. I love their new Wheel of Fortune game! On our first play, we ended up winning a little over 700 tickets! They also have Fruit Ninja as a game. It was fun with the big widescreen but unfortunately you don't win as many tickets as you would hope. I wish their ticket center would have individual ticket counters like the other D&B's I've been to, where you feed a counting machine your tickets. At this place, they just weigh your tickets and give you the final count. We didn't try the food here and instead went to P.F. Chang's to eat before coming here, but the Eat & Play deal looked fun. Too bad it was only restricted to certain times of the day. Overall I had a blast here and definitely would come back. Hopefully next time when there is a deal or promotion.\n",
      "\n",
      "Score: 3\n",
      "\n",
      "Prediction: 3\n",
      "\n",
      "\n",
      "Yelp Comments: To keep it short and sweet: Save yourself $100. Buy a good board game, your alcohol of choice, order a pizza, and invite your friends over. \\n\\nWhat an incredible disappointment. After seeing the enticing commercials so many times, we decided to give this place a try on a double date. I understand the prices of the play cards and won't dispute them; however, the food was incredibly over-priced, came out COLD (as in, sat on a counter without warmers for a minimum of 30 minutes) and I literally had to ask the bartender if there was any vodka in my drink. It was pure juice. $38 for three shots that had little-no alcohol in them. (Not to mention, my glass was dirty, and I saw the bartender scoop the glass into the ice basin because she was too lazy to use the sanitary scoop. I know the Food and Beverage Commission would be as disappointed as I was.) The service was terrible. Don't ask for anything from your waiter, as they are a little too busy on their cell phones or conversing amongst themselves. \\n\\nWas it fun to be in an adult-themed arcade? Yes. If you're looking for a good atmosphere to go with friends to play games, I suppose I would advise you give it a shot. I would never recommend their food, customer service, or drinks. Save yourself the money and stay home, or go for a traditional bowling, figure skating, roller-blading, rock climbing, basically any other physically-entertaining themed date instead.\n",
      "\n",
      "Score: 0\n",
      "\n",
      "Prediction: 0\n",
      "\n",
      "\n",
      "Yelp Comments: Microbrewed beers are only a plus to me when they're better than the beer I would have drank otherwise.  One waitress I had while there tried to split the uprights between macrobrew and trendy by describing one of their beers as \\\"kind of like Coors Light\\\".  If I wanted Coors, I'll pay half as much and have one.  \\n\\nThe food is actually a saving grace, as it was pretty good.  Head next door to Sing Sing for drinks, though.\n",
      "\n",
      "Score: 1\n",
      "\n",
      "Prediction: 2\n",
      "\n",
      "\n",
      "Yelp Comments: This is literally the first place I went when I first came to Pittsburgh - and I loved it. Hearty, full-sized meals, lots to choose from, and not too crowded or noisy. Although, for the sake of full disclosure, I didn't sit at the bar and I don't drink alcohol so I can't comment on that. I go back to Pittsburgh often and Rock Bottom ends up being a good standby for a dinner and a movie kind of night. Really, I have no complaints about this place so I'm surprised by the few low ratings here.\n",
      "\n",
      "Score: 3\n",
      "\n",
      "Prediction: 3\n",
      "\n",
      "\n",
      "Yelp Comments: Wast there last Friday. Seats right in front if the stage. The show was good. The headliner, while a bit long, was good. Fantastic service from our waitresses. Will definitely go back.\n",
      "\n",
      "Score: 3\n",
      "\n",
      "Prediction: 3\n",
      "\n",
      "\n",
      "Yelp Comments: Great Barnes and Noble location, and they have plenty of books that'll help you pass the time.\n",
      "\n",
      "Score: 2\n",
      "\n",
      "Prediction: 3\n",
      "\n",
      "\n",
      "Yelp Comments: The words \\\"epic fail\\\" get thrown around a lot these days....but I really feel like they apply in this situation.\\n\\nWe went on Friday, April 2 and arrived at 5:10pm.  It was busy, but not crowded....no waiting for a table.  Half price apps and drinks -- we ordered at 5:20.\\n\\nThe food (just appetizers, mind you...) didn't arrive till almost 6pm.  Drinks were ordered and were unbelievably slow.  We placed one order for 6 draft beers at 6:15...they arrived at 6:52.  *37 minutes for beer.*\\n\\nBy 7:00, we were canceling food orders which we'd given up on after waiting almost an hour.  We just wanted to leave.\\n\\nInstead of comping anything, they added 18% gratuity to each of our bills.  Uhhhh yeah.  Thanks for giving me one more reason never to come back.   We finally were able to leave around 7:30.\\n\\nBar Louie:  You are dead to me.\n",
      "\n",
      "Score: 0\n",
      "\n",
      "Prediction: 0\n",
      "\n",
      "\n",
      "Yelp Comments: I was looking to get out of the apartment on a really nice, sunny day and we decided to drive to Waterfront and walk around. All around great day until we hit Bar Louie for some drinks and appetizers.\\n\\nI am giving it one star, though it deserves none, because our waitress was nice, if a bit inattentive, and the hummus app we ordered was pretty damn good. Outside of that Bar Louie leaves a lot to be desired.\\n\\nThis is probably the first place I've been to where they charge $10 and up for all mixed drinks. To me, that is beyond ridiculous. Bar Louie is not an upscale restaurant, as much as they wish they were, and paying almost $60 for three drinks and two appetizers is insanity defined. \\n\\nAs I mentioned the waitress was friendly, but she definitely did not come back and check on us enough. I really wanted to like this place because of things I had heard and the drinks we had were really damn good, but if it want to spend that kind of money, I'll go somewhere that I can get good service.\n",
      "\n",
      "Score: 0\n",
      "\n",
      "Prediction: 1\n",
      "\n",
      "\n",
      "Yelp Comments: This place was pretty good food, but the service however was just horrible. As a friend said they hire for looks, not work ethic. Plus the fact that it took forty minutes just to get a couple appetizers was ridiculous. Our server seemed more interested in the table full of girls than actually waiting on us. The couple behind us came in the same time and wasn't even attended too until after OUR appetizers were being cleared off. \\n\\nThe food was pretty good, but because of the service I won't be returning.\n",
      "\n",
      "Score: 2\n",
      "\n",
      "Prediction: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_index = range(10,20)\n",
    "for i in test_index:\n",
    "    selected_sentence = dataset['test'][i]['text']\n",
    "    selected_label = dataset['test'][i]['label']\n",
    "    selected_tokenized_sentence = tokenizer(selected_sentence, return_tensors = 'pt').to(inference_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = inference_model(**selected_tokenized_sentence).logits\n",
    "        \n",
    "    predict_label = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(f\"Yelp Comments: {selected_sentence}\\n\")\n",
    "    print(f\"Score: {selected_label}\\n\")\n",
    "    print(f\"Prediction: {predict_label}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57eca69-9881-403c-844e-6685da539463",
   "metadata": {},
   "source": [
    "## Comapre two Model with metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab161b2-2120-4ab1-b05f-27f860caeba8",
   "metadata": {},
   "source": [
    "| Metric                | GPT-2 (Original) | GPT-2 PEFT (LoRA) |\n",
    "|-----------------------|------------------|-------------------|\n",
    "| **Eval Loss**         | 0.8052           | 0.8747            |\n",
    "| **Eval Accuracy**     | 64.66%           | 61.432%           |\n",
    "| **Eval F1 Score**     | 64.6214%         | 61.3061%          |\n",
    "| **Eval Precision**    | 64.8136%         | 61.2349%          |\n",
    "| **Eval Recall**       | 64.66%           | 61.432%           |\n",
    "| **Eval Runtime (s)**  | 75.8732          | 198.6707          |\n",
    "| **Samples per Second**| 65.899           | 251.673           |\n",
    "| **Steps per Second**  | 4.125            | 7.867             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badbc81c-0453-4772-92f7-801bb1936c2d",
   "metadata": {},
   "source": [
    "## Observation\n",
    "1. Performance: GPT-2 (Original) shows better loss, accuracy, F1 score, precision, and recall compared to GPT-2 PEFT (LoRA).\n",
    "2. GPT-2 PEFT (LoRA) processes samples and steps more quickly than the original GPT-2. This suggests that while PEFT (LoRA) takes longer overall, it handles more data per unit time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
